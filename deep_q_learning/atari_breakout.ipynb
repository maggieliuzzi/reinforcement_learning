{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep-Q-Learning.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMiI8eXzKq2XwUUkXTCbeV+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maggieliuzzi/reinforcement_learning/blob/master/deep_q_learning/atari_breakout.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0UBRY5XU4gSp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# https://deeplearningcourses.com/c/deep-reinforcement-learning-in-python\n",
        "\n",
        "# pip install gym[atari]\n",
        "from __future__ import print_function, division\n",
        "from builtins import range\n",
        "import copy\n",
        "import gym\n",
        "import os\n",
        "import sys\n",
        "import random\n",
        "import numpy as np\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf  # TensorFlow 1\n",
        "print(tf.__version__)\n",
        "import matplotlib.pyplot as plt\n",
        "from gym import wrappers\n",
        "from datetime import datetime\n",
        "!pip install scipy==1.2.0\n",
        "from scipy.misc import imresize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RU9pYZ4M4ncR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##### testing only\n",
        "# MAX_EXPERIENCES = 10000\n",
        "# MIN_EXPERIENCES = 1000\n",
        "\n",
        "MAX_EXPERIENCES = 500000\n",
        "MIN_EXPERIENCES = 50000\n",
        "TARGET_UPDATE_PERIOD = 10000\n",
        "IM_SIZE = 84\n",
        "K = 4 #env.action_space.n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTHbfdQR4toM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transform raw images for input into neural network\n",
        "# 1) Convert to grayscale\n",
        "# 2) Resize\n",
        "# 3) Crop\n",
        "class ImageTransformer:\n",
        "  def __init__(self):\n",
        "    with tf.variable_scope(\"image_transformer\"):\n",
        "      self.input_state = tf.placeholder(shape=[210, 160, 3], dtype=tf.uint8)\n",
        "      self.output = tf.image.rgb_to_grayscale(self.input_state)\n",
        "      self.output = tf.image.crop_to_bounding_box(self.output, 34, 0, 160, 160)\n",
        "      self.output = tf.image.resize_images(\n",
        "        self.output,\n",
        "        [IM_SIZE, IM_SIZE],\n",
        "        method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n",
        "      self.output = tf.squeeze(self.output)\n",
        "\n",
        "  def transform(self, state, sess=None):\n",
        "    sess = sess or tf.get_default_session()\n",
        "    return sess.run(self.output, { self.input_state: state })"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGhptqum4uIi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_state(state, obs_small):\n",
        "  return np.append(state[:,:,1:], np.expand_dims(obs_small, 2), axis=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfl_lCyN4uVO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ReplayMemory:\n",
        "  def __init__(self, size=MAX_EXPERIENCES, frame_height=IM_SIZE, frame_width=IM_SIZE, \n",
        "               agent_history_length=4, batch_size=32):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        size: Integer, Number of stored transitions\n",
        "        frame_height: Integer, Height of a frame of an Atari game\n",
        "        frame_width: Integer, Width of a frame of an Atari game\n",
        "        agent_history_length: Integer, Number of frames stacked together to create a state\n",
        "        batch_size: Integer, Number of transitions returned in a minibatch\n",
        "    \"\"\"\n",
        "    self.size = size\n",
        "    self.frame_height = frame_height\n",
        "    self.frame_width = frame_width\n",
        "    self.agent_history_length = agent_history_length\n",
        "    self.batch_size = batch_size\n",
        "    self.count = 0\n",
        "    self.current = 0\n",
        "    \n",
        "    # Pre-allocate memory\n",
        "    self.actions = np.empty(self.size, dtype=np.int32)\n",
        "    self.rewards = np.empty(self.size, dtype=np.float32)\n",
        "    self.frames = np.empty((self.size, self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "    self.terminal_flags = np.empty(self.size, dtype=np.bool)\n",
        "    \n",
        "    # Pre-allocate memory for the states and new_states in a minibatch\n",
        "    self.states = np.empty((self.batch_size, self.agent_history_length, \n",
        "                            self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "    self.new_states = np.empty((self.batch_size, self.agent_history_length, \n",
        "                                self.frame_height, self.frame_width), dtype=np.uint8)\n",
        "    self.indices = np.empty(self.batch_size, dtype=np.int32)\n",
        "      \n",
        "  def add_experience(self, action, frame, reward, terminal):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        action: An integer-encoded action\n",
        "        frame: One grayscale frame of the game\n",
        "        reward: reward the agend received for performing an action\n",
        "        terminal: A bool stating whether the episode terminated\n",
        "    \"\"\"\n",
        "    if frame.shape != (self.frame_height, self.frame_width):\n",
        "      raise ValueError('Dimension of frame is wrong!')\n",
        "    self.actions[self.current] = action\n",
        "    self.frames[self.current, ...] = frame\n",
        "    self.rewards[self.current] = reward\n",
        "    self.terminal_flags[self.current] = terminal\n",
        "    self.count = max(self.count, self.current+1)\n",
        "    self.current = (self.current + 1) % self.size\n",
        "           \n",
        "  def _get_state(self, index):\n",
        "    if self.count is 0:\n",
        "      raise ValueError(\"The replay memory is empty!\")\n",
        "    if index < self.agent_history_length - 1:\n",
        "      raise ValueError(\"Index must be min 3\")\n",
        "    return self.frames[index-self.agent_history_length+1:index+1, ...]\n",
        "      \n",
        "  def _get_valid_indices(self):\n",
        "    for i in range(self.batch_size):\n",
        "      while True:\n",
        "        index = random.randint(self.agent_history_length, self.count - 1)\n",
        "        if index < self.agent_history_length:\n",
        "          continue\n",
        "        if index >= self.current and index - self.agent_history_length <= self.current:\n",
        "          continue\n",
        "        if self.terminal_flags[index - self.agent_history_length:index].any():\n",
        "          continue\n",
        "        break\n",
        "      self.indices[i] = index\n",
        "          \n",
        "  def get_minibatch(self):\n",
        "    \"\"\"\n",
        "    Returns a minibatch of self.batch_size transitions\n",
        "    \"\"\"\n",
        "    if self.count < self.agent_history_length:\n",
        "      raise ValueError('Not enough memories to get a minibatch')\n",
        "    \n",
        "    self._get_valid_indices()\n",
        "        \n",
        "    for i, idx in enumerate(self.indices):\n",
        "      self.states[i] = self._get_state(idx - 1)\n",
        "      self.new_states[i] = self._get_state(idx)\n",
        "    \n",
        "    return np.transpose(self.states, axes=(0, 2, 3, 1)), self.actions[self.indices], self.rewards[self.indices], np.transpose(self.new_states, axes=(0, 2, 3, 1)), self.terminal_flags[self.indices]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ireZ6gIY4yXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DQN:\n",
        "  def __init__(self, K, conv_layer_sizes, hidden_layer_sizes, scope):\n",
        "\n",
        "    self.K = K\n",
        "    self.scope = scope\n",
        "\n",
        "    with tf.variable_scope(scope):\n",
        "\n",
        "      # inputs and targets\n",
        "      self.X = tf.placeholder(tf.float32, shape=(None, IM_SIZE, IM_SIZE, 4), name='X')\n",
        "\n",
        "      # tensorflow convolution needs the order to be:\n",
        "      # (num_samples, height, width, \"color\")\n",
        "\n",
        "      self.G = tf.placeholder(tf.float32, shape=(None,), name='G')\n",
        "      self.actions = tf.placeholder(tf.int32, shape=(None,), name='actions')\n",
        "\n",
        "      # calculate output and cost\n",
        "      # convolutional layers\n",
        "      # these built-in layers are faster and don't require us to\n",
        "      # calculate the size of the output of the final conv layer!\n",
        "      Z = self.X / 255.0\n",
        "      for num_output_filters, filtersz, poolsz in conv_layer_sizes:\n",
        "        Z = tf.contrib.layers.conv2d(\n",
        "          Z,\n",
        "          num_output_filters,\n",
        "          filtersz,\n",
        "          poolsz,\n",
        "          activation_fn=tf.nn.relu\n",
        "        )\n",
        "\n",
        "      # fully connected layers\n",
        "      Z = tf.contrib.layers.flatten(Z)\n",
        "      for M in hidden_layer_sizes:\n",
        "        Z = tf.contrib.layers.fully_connected(Z, M)\n",
        "\n",
        "      # final output layer\n",
        "      self.predict_op = tf.contrib.layers.fully_connected(Z, K)\n",
        "\n",
        "      selected_action_values = tf.reduce_sum(\n",
        "        self.predict_op * tf.one_hot(self.actions, K),\n",
        "        reduction_indices=[1]\n",
        "      )\n",
        "\n",
        "      # cost = tf.reduce_mean(tf.square(self.G - selected_action_values))\n",
        "      cost = tf.reduce_mean(tf.losses.huber_loss(self.G, selected_action_values))\n",
        "      self.train_op = tf.train.AdamOptimizer(1e-5).minimize(cost)\n",
        "      # self.train_op = tf.train.AdagradOptimizer(1e-2).minimize(cost)\n",
        "      # self.train_op = tf.train.RMSPropOptimizer(2.5e-4, decay=0.99, epsilon=1e-3).minimize(cost)\n",
        "      # self.train_op = tf.train.RMSPropOptimizer(0.00025, 0.99, 0.0, 1e-6).minimize(cost)\n",
        "      # self.train_op = tf.train.MomentumOptimizer(1e-3, momentum=0.9).minimize(cost)\n",
        "      # self.train_op = tf.train.GradientDescentOptimizer(1e-4).minimize(cost)\n",
        "\n",
        "      self.cost = cost\n",
        "\n",
        "\n",
        "  def copy_from(self, other):\n",
        "    mine = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
        "    mine = sorted(mine, key=lambda v: v.name)\n",
        "    theirs = [t for t in tf.trainable_variables() if t.name.startswith(other.scope)]\n",
        "    theirs = sorted(theirs, key=lambda v: v.name)\n",
        "\n",
        "    ops = []\n",
        "    for p, q in zip(mine, theirs):\n",
        "      op = p.assign(q)\n",
        "      ops.append(op)\n",
        "    self.session.run(ops)\n",
        "\n",
        "\n",
        "  def save(self):\n",
        "    params = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
        "    params = self.session.run(params)\n",
        "    np.savez('tf_dqn_weights.npz', *params)\n",
        "\n",
        "\n",
        "  def load(self):\n",
        "    params = [t for t in tf.trainable_variables() if t.name.startswith(self.scope)]\n",
        "    npz = np.load('tf_dqn_weights.npz')\n",
        "    ops = []\n",
        "    for p, (_, v) in zip(params, npz.iteritems()):\n",
        "      ops.append(p.assign(v))\n",
        "    self.session.run(ops)\n",
        "\n",
        "\n",
        "  def set_session(self, session):\n",
        "    self.session = session\n",
        "\n",
        "  def predict(self, states):\n",
        "    return self.session.run(self.predict_op, feed_dict={self.X: states})\n",
        "\n",
        "  def update(self, states, actions, targets):\n",
        "    c, _ = self.session.run(\n",
        "      [self.cost, self.train_op],\n",
        "      feed_dict={\n",
        "        self.X: states,\n",
        "        self.G: targets,\n",
        "        self.actions: actions\n",
        "      }\n",
        "    )\n",
        "    return c\n",
        "\n",
        "  def sample_action(self, x, eps):\n",
        "    if np.random.random() < eps:\n",
        "      return np.random.choice(self.K)\n",
        "    else:\n",
        "      return np.argmax(self.predict([x])[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y8szE4KJ4ya6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn(model, target_model, experience_replay_buffer, gamma, batch_size):\n",
        "  # Sample experiences\n",
        "  states, actions, rewards, next_states, dones = experience_replay_buffer.get_minibatch()\n",
        "\n",
        "  # Calculate targets\n",
        "  next_Qs = target_model.predict(next_states)\n",
        "  next_Q = np.amax(next_Qs, axis=1)\n",
        "  targets = rewards + np.invert(dones).astype(np.float32) * gamma * next_Q\n",
        "\n",
        "  # Update model\n",
        "  loss = model.update(states, actions, targets)\n",
        "  return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfLDebEL47Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_one(\n",
        "  env,\n",
        "  sess,\n",
        "  total_t,\n",
        "  experience_replay_buffer,\n",
        "  model,\n",
        "  target_model,\n",
        "  image_transformer,\n",
        "  gamma,\n",
        "  batch_size,\n",
        "  epsilon,\n",
        "  epsilon_change,\n",
        "  epsilon_min):\n",
        "\n",
        "  t0 = datetime.now()\n",
        "\n",
        "  # Reset the environment\n",
        "  obs = env.reset()\n",
        "  obs_small = image_transformer.transform(obs, sess)\n",
        "  state = np.stack([obs_small] * 4, axis=2)\n",
        "  loss = None\n",
        "\n",
        "\n",
        "  total_time_training = 0\n",
        "  num_steps_in_episode = 0\n",
        "  episode_reward = 0\n",
        "\n",
        "  done = False\n",
        "  while not done:\n",
        "\n",
        "    # Update target network\n",
        "    if total_t % TARGET_UPDATE_PERIOD == 0:\n",
        "      target_model.copy_from(model)\n",
        "      print(\"Copied model parameters to target network. total_t = %s, period = %s\" % (total_t, TARGET_UPDATE_PERIOD))\n",
        "\n",
        "\n",
        "    # Take action\n",
        "    action = model.sample_action(state, epsilon)\n",
        "    obs, reward, done, _ = env.step(action)\n",
        "    obs_small = image_transformer.transform(obs, sess)\n",
        "    next_state = update_state(state, obs_small)\n",
        "\n",
        "    # Compute total reward\n",
        "    episode_reward += reward\n",
        "\n",
        "    # Save the latest experience\n",
        "    experience_replay_buffer.add_experience(action, obs_small, reward, done)    \n",
        "\n",
        "    # Train the model, keep track of time\n",
        "    t0_2 = datetime.now()\n",
        "    loss = learn(model, target_model, experience_replay_buffer, gamma, batch_size)\n",
        "    dt = datetime.now() - t0_2\n",
        "\n",
        "    # More debugging info\n",
        "    total_time_training += dt.total_seconds()\n",
        "    num_steps_in_episode += 1\n",
        "\n",
        "\n",
        "    state = next_state\n",
        "    total_t += 1\n",
        "\n",
        "    epsilon = max(epsilon - epsilon_change, epsilon_min)\n",
        "\n",
        "  return total_t, episode_reward, (datetime.now() - t0), num_steps_in_episode, total_time_training/num_steps_in_episode, epsilon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRN9hHaU47bB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def smooth(x):\n",
        "  # last 100\n",
        "  n = len(x)\n",
        "  y = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    start = max(0, i - 99)\n",
        "    y[i] = float(x[start:(i+1)].sum()) / (i - start + 1)\n",
        "  return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GSLVvj4X5ARS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparams and initialize stuff\n",
        "conv_layer_sizes = [(32, 8, 4), (64, 4, 2), (64, 3, 1)]  # n filters, kernel size, stride\n",
        "hidden_layer_sizes = [512]  # size of each hidden layer that follows the convolutional layers\n",
        "gamma = 0.99\n",
        "batch_sz = 32\n",
        "num_episodes = 3500\n",
        "total_t = 0\n",
        "experience_replay_buffer = ReplayMemory()\n",
        "episode_rewards = np.zeros(num_episodes)\n",
        "\n",
        "# epsilon\n",
        "# decays linearly until 0.1\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.1\n",
        "epsilon_change = (epsilon - epsilon_min) / 500000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RGk3uayS5AVr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create environment\n",
        "env = gym.envs.make(\"Breakout-v0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjvnfnzQ5Jbm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create models\n",
        "model = DQN(\n",
        "  K=K,\n",
        "  conv_layer_sizes=conv_layer_sizes,\n",
        "  hidden_layer_sizes=hidden_layer_sizes,\n",
        "  scope=\"model\")\n",
        "target_model = DQN(\n",
        "  K=K,\n",
        "  conv_layer_sizes=conv_layer_sizes,\n",
        "  hidden_layer_sizes=hidden_layer_sizes,\n",
        "  scope=\"target_model\"\n",
        ")\n",
        "image_transformer = ImageTransformer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "77COc2Ai5G5W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "2abfa7f1-2bc1-4f40-be2b-eeddf8421454"
      },
      "source": [
        "with tf.Session() as sess:\n",
        "  model.set_session(sess)\n",
        "  target_model.set_session(sess)\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "\n",
        "\n",
        "  print(\"Populating experience replay buffer...\")\n",
        "  obs = env.reset()\n",
        "\n",
        "  for i in range(MIN_EXPERIENCES):\n",
        "\n",
        "      action = np.random.choice(K)\n",
        "      obs, reward, done, _ = env.step(action)\n",
        "      obs_small = image_transformer.transform(obs, sess) # not used anymore\n",
        "      experience_replay_buffer.add_experience(action, obs_small, reward, done)\n",
        "\n",
        "      if done:\n",
        "          obs = env.reset()\n",
        "\n",
        "\n",
        "  # Play a number of episodes and learn!\n",
        "  t0 = datetime.now()\n",
        "  for i in range(num_episodes):\n",
        "\n",
        "    total_t, episode_reward, duration, num_steps_in_episode, time_per_step, epsilon = play_one(\n",
        "      env,\n",
        "      sess,\n",
        "      total_t,\n",
        "      experience_replay_buffer,\n",
        "      model,\n",
        "      target_model,\n",
        "      image_transformer,\n",
        "      gamma,\n",
        "      batch_sz,\n",
        "      epsilon,\n",
        "      epsilon_change,\n",
        "      epsilon_min,\n",
        "    )\n",
        "    episode_rewards[i] = episode_reward\n",
        "\n",
        "    last_100_avg = episode_rewards[max(0, i - 100):i + 1].mean()\n",
        "    print(\"Episode:\", i,\n",
        "      \"Duration:\", duration,\n",
        "      \"Num steps:\", num_steps_in_episode,\n",
        "      \"Reward:\", episode_reward,\n",
        "      \"Training time per step:\", \"%.3f\" % time_per_step,\n",
        "      \"Avg Reward (Last 100):\", \"%.3f\" % last_100_avg,\n",
        "      \"Epsilon:\", \"%.3f\" % epsilon\n",
        "    )\n",
        "    sys.stdout.flush()\n",
        "  print(\"Total duration:\", datetime.now() - t0)\n",
        "\n",
        "  model.save()\n",
        "\n",
        "  # Plot the smoothed returns\n",
        "  y = smooth(episode_rewards)\n",
        "  plt.plot(episode_rewards, label='orig')\n",
        "  plt.plot(y, label='smoothed')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Populating experience replay buffer...\n",
            "Copied model parameters to target network. total_t = 0, period = 10000\n",
            "Episode: 0 Duration: 0:00:31.933950 Num steps: 245 Reward: 1.0 Training time per step: 0.128 Avg Reward (Last 100): 1.000 Epsilon: 1.000\n",
            "Episode: 1 Duration: 0:00:37.133979 Num steps: 289 Reward: 2.0 Training time per step: 0.126 Avg Reward (Last 100): 1.500 Epsilon: 0.999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}