# Explore-Exploit Dilemma

## Problems

- Multi-Armed Bandit

## Strategies

- Epsilon-greedy
    (Eg. 1, 5, 10%)

- Optimistic Initial Values
- UCB1 (Upper Confidence Bound)
- Thompson Sampling (Bayesian Bandit)


Note: some of this material was sourced from "Artificial Intelligence: Reinforcement Learning in Python" by "Lazy Programmer Inc".
